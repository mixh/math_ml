{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "764d0a27-a454-4e28-a5d6-9039a9fabbb3",
   "metadata": {},
   "source": [
    "# Q1. Housing Price Prediction\n",
    "\n",
    "Dataset Description: The data pertains to the houses found in each California district and some summary statistics about them based on the 1990 census data. It contains one instance per district block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). \n",
    "\n",
    "The goal of this task is to design a regression model to predict the median house value conditioned upon a set of input attributes corresponding to a particular California district block. \n",
    "\n",
    "The attributes in the dataset are as follows; their names are self-explanatory: \n",
    "     \n",
    "\n",
    "    longitude (continuous): One of the coordinates that are used to identify the California district block \n",
    "     \n",
    "\n",
    "    latitude (continuous): One of the coordinates that are used to identify the California district block \n",
    "     \n",
    "\n",
    "    housing_median_age (continuous): Average age of the house in California district block \n",
    "     \n",
    "\n",
    "    total_rooms (continuous): Total number of rooms of all the houses in the California district block \n",
    "     \n",
    "\n",
    "    total_bedrooms (continuous): Total number of bedrooms of all the houses in the California district block \n",
    "     \n",
    "\n",
    "    population (continuous): Number of people residing in the district block \n",
    "     \n",
    "\n",
    "    households (continuous): Number of families in the district block \n",
    "     \n",
    "\n",
    "    median_income (continuous): Median income for households in the district block of houses (measured in tens of thousands of US Dollars)  \n",
    "     \n",
    "\n",
    "    ocean_proximity (categorical): Location of the house. Is it inland, near the bay, near the ocean, etc.  \n",
    "     \n",
    "\n",
    "    median_house_value.(continuous): Median house value within a district block (measured in US Dollars)\n",
    "\n",
    "Our target variable will be median_house_value.  Use the rest of the fields mentioned above to predict the median_house_value. \n",
    "\n",
    "## Import Libraries\n",
    "Import all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3de6346-1836-4312-8f21-777ac6b59e4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422ee179-f70a-4a88-8b44-117deb549881",
   "metadata": {},
   "source": [
    "### b. Data Loading / Preprocessing\n",
    "\n",
    "#### i. Loading\n",
    "\n",
    "1. Load the California housing dataset using `pandas.read_csv()` function and store it in the variable (i.e., a pandas dataframe) named `df’.\n",
    "\n",
    "2. The resulting data frame should have the shape (20640, 10) indicating that there are 20640 rows and 10 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0715c8b5-c895-45a7-b3b7-f8ba38fef039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0c2c788-2ad1-4cb8-b78a-435562a4cb68",
   "metadata": {},
   "source": [
    "3. Find the missing values in the data frame. If any (i.e., even if one column in each instance / row has a missing value), drop the row using `pandas.DataFrame.dropna()` function. The resulting data frame should have the shape (20433, 10) indicating that there are 20433 rows and 10 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e1a1f1-74fa-4710-95c4-030843a2d873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1dcdaee1-691e-47ae-b90d-0cda36ccf0cd",
   "metadata": {},
   "source": [
    "4. Create a data frame `corr_df` by dropping the columns latitude, longitude, and ocean_proximity using the `pandas.DataFrame.drop()` function. Use the Pearson correlation to find the correlation of each remaining feature in the `corr_df` with the target variable `median_house_value` using the function `pandas.DataFrame.corrwith()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0b9cb4-39c9-4f08-859c-08e7d4a587b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e5fa925-788d-45a7-bfd4-9d688abb3696",
   "metadata": {},
   "source": [
    "5. Create a data frame `X` of features (by dropping the column `median_house_value` from the original data frame) using the `pandas.DataFrame.drop()` function. Create a Series object of targets `Y` (by only considering the `median_house_value` column from the original data frame (Do NOT use the `corr_df` data frame in this step. Use the data frame which was obtained as a result of step 3 above). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c01708-a2c9-4221-92a5-5955ba328ff8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46ddbc8b-dcf5-46ff-b4c6-eb02429318e7",
   "metadata": {},
   "source": [
    "#### ii. Data Visualization\n",
    "\n",
    "1. Use `pandas.DataFrame.hist(bins = 50)` function for visualizing the variation on the columns housing_median_age, total_rooms, total_bedrooms, population, household, median_income and median_house_value. Plot each histogram as a separate subplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff0dd03-191c-4264-8265-9f861f0e32b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf638e98-eb4a-4fd6-9b12-251eb3d2ce00",
   "metadata": {},
   "source": [
    "2. Use `pandas.dataframe.describe()` function to find the mean, median and standard deviations for each feature and report in the jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4993254e-ebfb-4f22-98d7-2e646c7e53d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc7d7c60-763c-4487-8ab7-3c194ca384a9",
   "metadata": {},
   "source": [
    "3. Use `pandas.get_dummies` to convert categorical variables into dummy /one-hot encoding. In this case the categorical column is ocean_proximity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea65923-0d3d-465e-8c1c-fea0e11416b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3de9798-341c-4fad-bb69-e07655a5859c",
   "metadata": {},
   "source": [
    "#### iii. Data Splitting\n",
    "\n",
    "1. Split data into training and test sets using the sklearn `train_test_split()` function. Perform 70-30 distribution i.e. 70% training and 30% testing. The result of your data split should yield 4 separate data frames `X_train, X_test, y_train, y_test`. (respectively, the training features, testing features, training targets and testing target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2000c4f2-41e6-49d3-807e-c77b86c4e60c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc6cdeca-0f98-4039-8191-61cef7338869",
   "metadata": {},
   "source": [
    "#### iv. Data Scaling\n",
    "\n",
    "1. Use the `StandardScaler()` to instantiate the standard scaler class. Note: You will need two separate scaler objects, one to scale the features, another to scale the target values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11372a31-8e9d-4d9c-b79e-925cbcc1d752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3b7c211-b7eb-4778-9145-9d23da103eb9",
   "metadata": {},
   "source": [
    "2. For each scaler, employ the `fit_transform()` function (only on the training  features, training targets) of the scaler to retrieve the new (scaled) version of the data. Store them in `X_train`, and `y_train` again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f826b031-b5eb-481b-83a3-1533b766cdf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5131ddd-6593-4ad3-b9aa-47635785d50f",
   "metadata": {},
   "source": [
    "3. Scale the `X_test` and `y_test` as well and store the scaled values back in `X_test` and `y_test`. (i.e., use the appropriate “fitted” scaler above to “transform” the test data. Note: the function to be employed in this case is `transform()` as opposed to `fit_transform()`).  \n",
    "Henceforth, `X_train, y_train, X_test, y_test` will refer to the scaled data unless stated otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb9a66d-7b9e-4de9-a8ec-3dcadef69bc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32c0d0b8-b8af-42b7-ba48-f4b50b3ce721",
   "metadata": {},
   "source": [
    "4. Use `pandas.DataFrame.hist(bins = 50)` function for visualizing the variation of numerical attributes housing_median_age, total_rooms, total_bedrooms, population, household, median_income and median_house_value for the `X_train` and `y_train` dataset (similar to step b.ii.1 above). Once again, plot each histogram as a separate subplot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc940a87-f29d-429a-9728-6d4b40c2b5ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d85dae5-dd57-4e72-9761-b2cd10dbd486",
   "metadata": {},
   "source": [
    "### c. Modelling\n",
    "\n",
    "#### i. Employ Linear Regression from sklearn.linear_model, and instantiate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175f360a-0218-4f6a-b561-cde7c1a85b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b16fc923-4ef7-4234-b2f4-590a6a38f667",
   "metadata": {},
   "source": [
    "#### ii. Once instantiated, `fit()` the model using the scaled `X_train, y_train` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ed32ed-22ef-4abb-a26c-c7734f0b68a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a09f41e-46b4-4f0b-9f63-5aa6e35eca7b",
   "metadata": {},
   "source": [
    "#### iii. Employ the `predict()` function to obtain predictions on `X_test`. Store the predictions in a variable named `y_preds`. Note: Since the model has been trained on scaled data (i.e., both features and targets, the predictions will also be in the “scaled” space. We need to transform the predictions back to the original space). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96ff705-4922-472b-a885-00bd822c9b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4330dab2-1121-4abe-8782-94f83ee2ec1f",
   "metadata": {},
   "source": [
    "#### iv. Use `inverse_transform()` function to convert the normalized data (`y_preds` ) to original scale. Store the transformed values back into `y_preds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b22039-ba77-4e6f-a898-b440a9f2d197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69a63391-8b85-449a-8f5f-813f5d640836",
   "metadata": {},
   "source": [
    "#### v. Perform PCA on the features (`X_train`) and set `n_component` as 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939778f8-744d-470d-9d56-e4a6d73f8475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "982b67bb-4311-48bd-83d8-8afd4a3de3f3",
   "metadata": {},
   "source": [
    "1. Show a scatter plot where on the x-axis we plot the first PCA component and second component on the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66588c88-4bc4-489d-a6a2-2d4e66ee0f43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e53d682-6844-4440-b669-608eecd92ef1",
   "metadata": {},
   "source": [
    "2. Calculate the total percentage of variance captured by the 2 PCA components using `pca.explained_variance_ratio_`. Also, report the strength of each PCA component using `pca.singular_values_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca58c930-1ddb-4d04-8484-c8c8590c28e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfa373cb-33f5-4fdb-9e82-6b5de5c18e16",
   "metadata": {},
   "source": [
    "### d. Evaluation\n",
    "\n",
    "#### i. Plot a scatter plot using matplotlib.pyplot.scatter function. Plot the predicted median house values on the y-axis vs the actual median house values on the x-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c45488f-1df8-4cc2-865d-5113ef7e6aea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "faf7711e-f2f3-443e-a0ce-05d7dc77c0c9",
   "metadata": {},
   "source": [
    "#### ii. Calculate MAPE, RMSE and R2 for the model and report them in the following table.  \n",
    "Hint: for RMSE set the squared parameter to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ffc813-3968-49d4-9477-7c37f96e50ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
